{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2d20bf-ad56-4057-b9ee-6bb654a2c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,TrainingArguments,Trainer,BitsAndBytesConfig,DataCollatorForLanguageModeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "accelerator = Accelerator()\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50acc4e-8912-41c9-90d0-67016bd18dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"function_call.jsonl\",lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa967acf-5e68-4e71-be90-e3b61ea32dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.choices(range(0,101),k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81655ef-aa5a-4f92-a9a7-cb887fd07fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 61, 88, 36, 99, 20, 71, 35, 63, 85]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e40590-62d5-4004-a700-87b5750d19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df.iloc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785b42f6-1dce-4c51-a52f-81fd0afcffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a47c632-4220-4e2e-b346-010b4b2bd0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff627f728b74c84a4e49e543afd3cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37d9dde-f5e0-4c7c-aff6-36375fd44852",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543e9b16-dc6f-4f25-b376-bf28455d14fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Reply with JSON for the following question: I want to do a total of 8945 and 1352',\n",
       " 'Here is your generated JSON: \\n```json\\n{\\n    \"function_name\": \"add\",\\n    \"parameter_1\": \"8945\",\\n    \"parameter_2\": \"1352\"\\n}\\n```')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'instruction'],df.loc[0,'output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c0f509-ae68-4b8f-94eb-a7fd4b424524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genPrompt(ex):\n",
    "    prompt = f\"\"\"<s>[INST]You are an Expert Computer Programmer.\n",
    "    You will Recieve a prompt to generate JSON for the given mathematical operation.\n",
    "    You can only return the numbers and the operation as a function as JSON.\n",
    "    Example:\n",
    "    - User: Reply with JSON for the following question: I want to do a total of 8945 and 1352\n",
    "    - Assistant: Here is your generated JSON: \\n```json\\n{{\\n    \"function_name\": \"add\",\\n    \"parameter_1\": \"8945\",\\n    \"parameter_2\": \"1352\"\\n}}\\n```'\n",
    "    [/INST]</s>\n",
    "    <s>\n",
    "    User:{ex['instruction']}\n",
    "    Assistant:{ex['output']}\n",
    "    </s>\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba588a7-204e-448f-ae87-fa6d020801b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ex):\n",
    "    return tokenizer(genPrompt(ex),truncation = True,padding = 'max_length',return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f5cdc0-a147-4778-9586-08f42652ead0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c372d58d18f24db5bef0008455d883d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = datasets.Dataset.from_pandas(df)\n",
    "prompts = df.map(tokenize,batched = True,batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f7a5b6-198b-49e1-8806-ca32a2a9823f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 91\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9618e50d-a029-40ad-84a4-3d812aca8355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ddf8100-e32b-43f8-bb4f-d487000fa5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4155606fbfd04058bda4967ae644b766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val = datasets.Dataset.from_pandas(val).map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2d757a-31c6-4b09-8af1-ff73d7a6a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88744422-de7c-4344-8556-34ee8451afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = prompts.remove_columns(['instruction','output','__index_level_0__'])\n",
    "val = val.remove_columns(['instruction','output','__index_level_0__'])\n",
    "prompts = prompts.with_format(\"torch\")\n",
    "val = val.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a166ce-86dd-4388-ac45-71fe1e018ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctFormat(ex):\n",
    "    ex['input_ids'] = ex['input_ids'].squeeze(dim = 0)\n",
    "    ex['attention_mask'] = ex['attention_mask'].squeeze(dim = 0)\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9375c9f-daf4-4c89-941f-81c8465b6e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0d10c9694b4f33be28c2f1def4564b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val = val.map(correctFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65481c8d-52fb-4941-981f-5931148894ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9f4d64-639e-4507-af59-2643e2d6187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=prompts,\n",
    "    eval_dataset=val,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"mistralMathTune\",\n",
    "        do_train=True,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size = 10,\n",
    "        gradient_accumulation_steps=1,\n",
    "        weight_decay = 0.05,\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=2e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=1,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=4,                # Save checkpoints every 50 steps\n",
    "        eval_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=1,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=None,\n",
    "        gradient_checkpointing = False,\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "607b906e-ff61-4a04-bf96-f374d0ef19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 1:36:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.409521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.403842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>0.398489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.392871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.381754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.374038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.373761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.374218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.246900</td>\n",
       "      <td>0.373374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.372184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.368626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.364933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.362784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.359792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.359615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.358092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.356817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.356286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.355379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.355245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.355637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.354139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.353354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.351030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.349062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.348724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.346883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.346576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.345937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.345756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.345960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.344676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.344959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.344909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.345058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.345601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.345215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.345998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.345489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.345709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/aayushv2001/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.1181545952335, metrics={'train_runtime': 5899.1545, 'train_samples_per_second': 0.154, 'train_steps_per_second': 0.014, 'total_flos': 1.994623719309312e+16, 'train_loss': 0.1181545952335, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train('mistralMathTune/checkpoint-40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b62afa9-c871-4942-88ef-cb57d6b0449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayushv2001/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040b7db810c142978988e5727bb99db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd025874-7dd7-4363-b23a-536669733265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(\"What is 12*2.5/6?\\nOnly give me the answer, I do not need the process.\",return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a73c070-d185-4452-823e-4ee96e71c2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"mistralMathTune/checkpoint-72\")\n",
    "ft_model.eval() \n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "496d5911-17cb-451e-8d18-4fc2333d055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 12*2.5/6?\n",
      "Only give me the answer, I do not need the process.\n",
      "ANSWER: 2\n",
      "EXPLANATION: The order of operations (PEMDAS) states that you should perform exponentiation before division or multiplication, and division before addition or subtraction. So in this problem, we first multiply 12 by 2.5 to get 30, then divide 30 by 6 to get 5. However, since all numbers are integers, there will be no decimal places when performing integer arith\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2,repetition_penalty = 1.2,do_sample = True,temperature = 0.1)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0699e80a-2919-46fb-9def-de81cac5d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(\"What is 50+125+49?\\nOnly give me the answer, I do not need the process.\",return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a33c8404-2679-41e7-986c-08105e894ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 50+125+49?\n",
      "Only give me the answer, I do not need the process.\n",
      "ANSWER: The sum of 50 + 125 + 49 equals 224.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2,repetition_penalty = 1.2,do_sample = True,temperature = 0.1)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407c211-4ab9-44db-b12a-98da8f44f4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
